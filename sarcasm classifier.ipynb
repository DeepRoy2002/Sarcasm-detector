{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04aa732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 08:43:24.919895: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#tensorflow is an open source library developed by google\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6d2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('Sarcasm_Headlines_Dataset.json',lines=True)\n",
    "#read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada11f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0df4052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "0      former versace store clerk sues over secret 'b...             0\n",
       "1      the 'roseanne' revival catches up to our thorn...             0\n",
       "2      mom starting to fear son's web series closest ...             1\n",
       "3      boehner just wants wife to listen, not come up...             1\n",
       "4      j.k. rowling wishes snape happy birthday in th...             0\n",
       "...                                                  ...           ...\n",
       "26704               american politics in moral free-fall             0\n",
       "26705                            america's best 20 hikes             0\n",
       "26706                              reparations and obama             0\n",
       "26707  israeli ban targeting boycott supporters raise...             0\n",
       "26708                  gourmet gifts for the foodie 2014             0\n",
       "\n",
       "[26709 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['article_link'], axis=1)\n",
    "#drop the axis column as it is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3509bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data['headline'].tolist()\n",
    "label = data['is_sarcastic'].tolist()\n",
    "#convert the dataframe to list to access them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8213c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       " 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       " \"advancing the world's women\",\n",
       " 'the fascinating case for eating lab-grown meat',\n",
       " 'this ceo will send your kids to school, if you work for his company',\n",
       " 'top snake handler leaves sinking huckabee campaign',\n",
       " \"friday's morning email: inside trump's presser for the ages\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0:10]\n",
    "#first 10 headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9672597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10]\n",
    "#first 10 labels 1- sarcastic, 0- not sarcastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191007b5",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be01e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(len(sentence)*0.75)\n",
    "# 75% data is used for training remaining 25% for testing\n",
    "train_sen = sentence[0:train_size]\n",
    "test_sen = sentence[train_size : ]\n",
    "train_lab = label[0:train_size]\n",
    "test_lab = label[train_size : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e105f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "oov_tok = \"oov\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fed38",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b24b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token = oov_tok)\n",
    "#num_words - the maximum no. of words to keep, only the most common num_words - 1 will be kept\n",
    "#oov_tok - If given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "tokenizer.fit_on_texts(train_sen)\n",
    "word_index = tokenizer.word_index\n",
    "#index of the tokens in dictionary format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560b028",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24176450",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100 #max length of a sentence can be 100, if not provided then the length of the longest sent.\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca0d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(train_sen)\n",
    "# assigning sequences , numbers to tokens as model can only train on numbers\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "# in any raw text data, naturally there will be sentences of difeerent lenths. all nn require th same input size\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_sen)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be962c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 08:43:29.381126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "#Sequential models are the machine learning models that input or output sequences of data, sequential data\n",
    "#includes text streams, audio clips, video clips etc.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    #input_dim - Size of the vocab\n",
    "    #output_dim - the no. of dimensions we want to embed into, each word will be represented in this much dimension\n",
    "    #input_length - length of the maximum document\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    #it adds up the vectors\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    # an activation fynction in nn defines how the weighted sum of the inputs is transformed into the output from a node or \n",
    "    #nodes in a layer of the network\n",
    "    # 24 - dimension of the output spa ce\n",
    "    # relu - rectified linear unit\n",
    "    # it is an activation function if the input is positive then the output will be the i/p else zero\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "    # takes any real values as input and outputs in the range of 0 to 1, the larger the input closer will it be to 1 \n",
    "    # and vice versa\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#binary_crossentropy - loss function used for binary stuffs\n",
    "#optimizer - adam - used to change the weights of the attritubutes of the nn to reduce losses\n",
    "model.summary()\n",
    "# embedding - batch_size, input_length, output_dim\n",
    "#gloavg - batch_size,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6098af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(train_lab)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(test_lab)\n",
    "#need to convert the lists to arrays for Tensorflow 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d6bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "626/626 - 1s - loss: 0.6745 - accuracy: 0.5648 - val_loss: 0.6211 - val_accuracy: 0.6322 - 1s/epoch - 2ms/step\n",
      "Epoch 2/30\n",
      "626/626 - 1s - loss: 0.4655 - accuracy: 0.8157 - val_loss: 0.3992 - val_accuracy: 0.8341 - 675ms/epoch - 1ms/step\n",
      "Epoch 3/30\n",
      "626/626 - 1s - loss: 0.3277 - accuracy: 0.8696 - val_loss: 0.3596 - val_accuracy: 0.8501 - 681ms/epoch - 1ms/step\n",
      "Epoch 4/30\n",
      "626/626 - 1s - loss: 0.2740 - accuracy: 0.8933 - val_loss: 0.3455 - val_accuracy: 0.8549 - 678ms/epoch - 1ms/step\n",
      "Epoch 5/30\n",
      "626/626 - 1s - loss: 0.2378 - accuracy: 0.9052 - val_loss: 0.3420 - val_accuracy: 0.8541 - 671ms/epoch - 1ms/step\n",
      "Epoch 6/30\n",
      "626/626 - 1s - loss: 0.2118 - accuracy: 0.9171 - val_loss: 0.3478 - val_accuracy: 0.8582 - 678ms/epoch - 1ms/step\n",
      "Epoch 7/30\n",
      "626/626 - 1s - loss: 0.1873 - accuracy: 0.9292 - val_loss: 0.3568 - val_accuracy: 0.8570 - 691ms/epoch - 1ms/step\n",
      "Epoch 8/30\n",
      "626/626 - 1s - loss: 0.1700 - accuracy: 0.9355 - val_loss: 0.3634 - val_accuracy: 0.8550 - 670ms/epoch - 1ms/step\n",
      "Epoch 9/30\n",
      "626/626 - 1s - loss: 0.1527 - accuracy: 0.9428 - val_loss: 0.3781 - val_accuracy: 0.8547 - 677ms/epoch - 1ms/step\n",
      "Epoch 10/30\n",
      "626/626 - 1s - loss: 0.1393 - accuracy: 0.9510 - val_loss: 0.3943 - val_accuracy: 0.8532 - 675ms/epoch - 1ms/step\n",
      "Epoch 11/30\n",
      "626/626 - 1s - loss: 0.1276 - accuracy: 0.9552 - val_loss: 0.4177 - val_accuracy: 0.8505 - 670ms/epoch - 1ms/step\n",
      "Epoch 12/30\n",
      "626/626 - 1s - loss: 0.1168 - accuracy: 0.9583 - val_loss: 0.4342 - val_accuracy: 0.8504 - 678ms/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "626/626 - 1s - loss: 0.1075 - accuracy: 0.9623 - val_loss: 0.4570 - val_accuracy: 0.8487 - 673ms/epoch - 1ms/step\n",
      "Epoch 14/30\n",
      "626/626 - 1s - loss: 0.0999 - accuracy: 0.9664 - val_loss: 0.4826 - val_accuracy: 0.8433 - 680ms/epoch - 1ms/step\n",
      "Epoch 15/30\n",
      "626/626 - 1s - loss: 0.0906 - accuracy: 0.9693 - val_loss: 0.5032 - val_accuracy: 0.8441 - 678ms/epoch - 1ms/step\n",
      "Epoch 16/30\n",
      "626/626 - 1s - loss: 0.0827 - accuracy: 0.9723 - val_loss: 0.5310 - val_accuracy: 0.8411 - 672ms/epoch - 1ms/step\n",
      "Epoch 17/30\n",
      "626/626 - 1s - loss: 0.0764 - accuracy: 0.9761 - val_loss: 0.5817 - val_accuracy: 0.8315 - 681ms/epoch - 1ms/step\n",
      "Epoch 18/30\n",
      "626/626 - 1s - loss: 0.0726 - accuracy: 0.9761 - val_loss: 0.6009 - val_accuracy: 0.8345 - 674ms/epoch - 1ms/step\n",
      "Epoch 19/30\n",
      "626/626 - 1s - loss: 0.0650 - accuracy: 0.9790 - val_loss: 0.6255 - val_accuracy: 0.8324 - 672ms/epoch - 1ms/step\n",
      "Epoch 20/30\n",
      "626/626 - 1s - loss: 0.0603 - accuracy: 0.9813 - val_loss: 0.6700 - val_accuracy: 0.8269 - 679ms/epoch - 1ms/step\n",
      "Epoch 21/30\n",
      "626/626 - 1s - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.6944 - val_accuracy: 0.8279 - 673ms/epoch - 1ms/step\n",
      "Epoch 22/30\n",
      "626/626 - 1s - loss: 0.0515 - accuracy: 0.9837 - val_loss: 0.7621 - val_accuracy: 0.8261 - 669ms/epoch - 1ms/step\n",
      "Epoch 23/30\n",
      "626/626 - 1s - loss: 0.0468 - accuracy: 0.9863 - val_loss: 0.8040 - val_accuracy: 0.8239 - 679ms/epoch - 1ms/step\n",
      "Epoch 24/30\n",
      "626/626 - 1s - loss: 0.0439 - accuracy: 0.9865 - val_loss: 0.8286 - val_accuracy: 0.8228 - 678ms/epoch - 1ms/step\n",
      "Epoch 25/30\n",
      "626/626 - 1s - loss: 0.0398 - accuracy: 0.9878 - val_loss: 0.8316 - val_accuracy: 0.8242 - 679ms/epoch - 1ms/step\n",
      "Epoch 26/30\n",
      "626/626 - 1s - loss: 0.0368 - accuracy: 0.9891 - val_loss: 0.8686 - val_accuracy: 0.8233 - 676ms/epoch - 1ms/step\n",
      "Epoch 27/30\n",
      "626/626 - 1s - loss: 0.0341 - accuracy: 0.9908 - val_loss: 0.9069 - val_accuracy: 0.8210 - 683ms/epoch - 1ms/step\n",
      "Epoch 28/30\n",
      "626/626 - 1s - loss: 0.0318 - accuracy: 0.9906 - val_loss: 0.9421 - val_accuracy: 0.8195 - 676ms/epoch - 1ms/step\n",
      "Epoch 29/30\n",
      "626/626 - 1s - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.9916 - val_accuracy: 0.8161 - 674ms/epoch - 1ms/step\n",
      "Epoch 30/30\n",
      "626/626 - 1s - loss: 0.0272 - accuracy: 0.9928 - val_loss: 1.0156 - val_accuracy: 0.8164 - 672ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "#An epoch in a neural network is the training of the neural network with all the training data for one cycle\n",
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)\n",
    "#verbose = 2 means one line per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ab679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "[[0.9996512 ]\n",
      " [0.01750244]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Coworkers At Bathroom sink locked in Tense Standoff Over Who is Going to wash hands longer\", \n",
    "            \"Spiking U.S. coronavirus cases could force rationing decisions similar to those made in Italy, China.\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaf647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
